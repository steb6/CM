\documentclass{article}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{ dsfont }
\usepackage{amssymb}
\usepackage{setspace}

% Questa linea raddoppia lo spazio tra una riga e l'altra per tutto il documento %
\doublespacing

\begin{document}
% Abstract %
\thispagestyle{plain}
\begin{center}
    \Large
    \textbf{Computational Mathematics for Learning and Data Analysis}
        
    \vspace{0.4cm}
    \large
    2019 / 2020
        
    \vspace{0.4cm}
    \textbf{Poggiali Alessandro, Berti Stefano}
       
    \vspace{0.9cm}
\end{center}

% Setting the stage %
\section{Setting the stage}\label{sec:setting-the-stage}

\subsection{The problem: Least Square}\label{subsec:linear-square-problem}
Our problem is to find an array x such that \\\\
\centerline{$\min_{x \in \mathds{R}^n}\|Ax-b\|$} \\\\
holds, where
\begin{itemize}
	\item $A$ is a \textit{tall thin matrix} (so it is a matrix $A\in M(m, n, R)$ where $m \gg n$)
	\item $b$ is a vector of real number
	\item $\|.\|$ is the \textit{2-norm} or \textit{Euclidean Norm}: $\|x\| := \sqrt{\sum_{i=1}^n x_i}$
\end{itemize}
and so to find the closest vector to $b$ inside the hyperplane $Im(A)$.

% Conjugate gradiend method %
\subsection{First algorithm: Conjugate Gradient Method}\label{subsec:conjugate-gradient-method}

\subsection{Second algorithm: QR factorization with Householder Reflectors}\label{subsec:qr-factorization-with-householder-reflectors}
To solve the Least Square Problem, we use the \textit{thin QR factorization} with all the optimization seen (fast Householder-vector product, cancellation problem resolution, manually changing known entries) in order to factorize $A$ as $Q_{1}R_{1}$.
We use a variant of the \textit{thin QR factorization} where we do not form the matrix $Q$, but we keep the Householder vector $u_{k}$ to perform implicit product with $Q$ and $Q^{T}$.
With this factorization, we can write $\|Ax - b\|$ as $\left\lVert \begin{bmatrix} R_{1}x - Q_{1}^{T}b \\ Q_{2}^{T}b\end{bmatrix} \right\lVert$
and now we can chose $x$ such that $R_{1}x - Q_{1}^{T}b = 0$, which is $x = R_{1}^{-1}Q_{1}^{T}b$ (if $R_{1}$ is invertible).
Finally we should have $x  = \arg\!\min_{x \in  \mathds{R}^n}\|Ax - b\| = R_{1}^{-1}Q_{1}^{T}b$ and $\|Ax - b\| = \|Q_{2}^{T}b\|$.


\section{What to expect from the algorithms}\label{sec:what-to-expect-from-the-algorithms}

\subsection{Conjugate Gradient}\label{subsec:conjugate-gradient}

\subsection{QR with HouseHolder}\label{subsec:qr-with-householder}
The QR factorization has a cancellation problem during the Householder reflector construction that is easily fixed by summing first entry and norm instead of subtract it.
% Apart for this problem, the factorization algorithm is \textit{backward stable}, because, at each step, every $\tilde{R_{k}}$, $\tilde{u_{k}}$ are the exact result that we would get if we started from $R_{k-1} + \Delta R_{K-1}$ with $\left\lVert \Delta R_{k-1} \right\rVert = O(u) \left\lVert R_{k-1} \right\rVert$
\\Apart from that, since every step is \textit{backward stable}, the factorization algorithm is \textit{backward stable}: the computed $Q$, $R$ are the exact result of $qr(A + \Delta A)$ where $ \left\lVert \Delta A \right\rVert \leq O(u)\left\lVert A \right\rVert$, so we only have intrinsic representation errors.
% We know that this algorithm is bandwith heavy and not parallelizable, as every reflection that produces a new zero element changes the entirety of both Q and R matrices.
% Modification of the algorithm %
\\The computational cost for thin QR factorization is $2mn^2 - \frac{2}{3}n^3 + O(mn)$ flops, which represents two cases: if $m \approx n$, we have that the cost is $\frac{4}{3}n^3$, if $m \gg n$ the cost scales like $2mn^2$, so it scales linearly with respect to the biggest dimension of A .
% As a comparision, LU/Gaussian elimination has computational cost of $\frac{2}{3}n^3$, but it is not a \textit{backward stable} algorithm.
\\Before we said that $x = R_{1}^{-1}Q_{1}^{T}b$, but to be able to calculate it we need $R_{1}$ to be invertible.
\\We know that $A$ has full column rank $\iff Az \neq 0 \forall z \neq 0 \iff z^{T}A^{T}Az = \left\lVert Az \right\rVert^{2} \forall z \neq 0 \iff A^{T}A$ is positive definite $\iff$ all eigenvalues of $A^{T}A$ are $> 0\iff $0 is not an eigenvalue of $A^{T}A \iff A^{T}A = (Q_{1}R_{1})^{T}Q_{1}R_{1} = R_{1}^{T}Q_{1}^{T}Q_{1}R_{1} = R_{1}^{T}R_{1}$ is invertible $\iff det(A^{T}A) = det(R_{1}^{T})det(R_{1}) \neq 0 \iff det(R_{1}) \neq 0 \iff R_{1}$ is invertible.
\\So if $A$ has full column rank, $\min_{x \in \mathds{R}^n}\|Ax-b\|$ has solution and it is unique.
\\$R_{1}$ is invertible also if all elements on its diagonal are $\neq 0$.

%If $A$ has full column rank,
%and $R_{1}$ is invertible $\iff A^{T}A = (Q_{1}R_{1})^{T}Q_{1}R_{1} = R_{1}^{T}Q_{1}^{T}Q_{1}R_{1} = R_{1}^{T}R_{1} \iff A^{T}A$ is positive definite $\iff$ A has full column rank.
The cost to solve $x = R_{1}^{-1}(Q_{1}^{T})b$ is a multiplication $c = (Q_{1}^{T})b$, which costs $O(mn)$, and the resolution of the triangular system $R_{1}x = c$ with back-substitution, which costs $O(n^{2})$, but the overall cost $O(mn) + O(n^{2})$ is negligible with respect to the cost $O(mn^{2})$ to compute $Q_{1}, R_{1}$.

\section{Input data}\label{sec:input-data}
Our input data is the matrix A, which is the tall thin matrix used as input in the \textit{ML-cup 2019-2020} competition.
Its shape is $(1765, 20)$, we augmented it with few functions of the features of the dataset:
\begin{itemize}
	\item \textbf{24\textsuperscript{th} column}: logarithm of the absolute value of the 1\textsuperscript{st} column
	\item \textbf{25\textsuperscript{th} column}: product of 2\textsuperscript{nd}, 3\textsuperscript{rd} and 4\textsuperscript{th} columns
	\item \textbf{26\textsuperscript{th} column}: 5\textsuperscript{th} column to square
\end{itemize}
So the final shape of $A$ is $(1765, 23)$.
\end{document}
