\documentclass{article}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{ dsfont }
\usepackage{amssymb}
\usepackage{setspace}

% Questa linea raddoppia lo spazio tra una riga e l'altra per tutto il documento %
\doublespacing

\begin{document}
% Abstract %
\thispagestyle{plain}
\begin{center}
    \Large
    \textbf{Computational Mathematics for Learning and Data Analysis}

    \vspace{0.4cm}
    \large
    2019 / 2020

    \vspace{0.4cm}
    \textbf{Poggiali Alessandro, Berti Stefano}

    \vspace{0.9cm}
\end{center}

% Setting the stage %
\section{Setting the stage}\label{sec:setting-the-stage}

\subsection{The problem: Least Square}\label{subsec:linear-square-problem}
Our problem is to find an array x such that \\\\
\centerline{$\min_{x \in \mathds{R}^n}\|Ax-b\|$} \\\\
holds, where
\begin{itemize}
	\item $A$ is a \textit{tall thin matrix} (so it is a matrix $A\in M(m, n, R)$ where $m \gg n$)
	\item $b$ is a vector of real number
	\item $\|.\|$ is the \textit{2-norm} or \textit{Euclidean Norm}: $\|x\| := \sqrt{\sum_{i=1}^n x_i}$
\end{itemize}
and so to find the closest vector to $b$ inside the hyperplane $Im(A)$.

% Conjugate gradiend method %
\subsection{First algorithm: Conjugate Gradient Method}\label{subsec:conjugate-gradient-method}
If a square matrix $A = A^{T}$ is positive definite, then we can find a solution to $Ax = b$ by minimizing the (strictly convex) function $f(x) = \frac{1}{2}x^{T}Ax - b^{T}x$ (that is equal to set $\nabla f(x) = Ax - b = 0$). However our matrix $A$ is a \emph{tall thin matrix} and since in this case finding a solution to  $Ax = b$ is equivalent to solving the normal equations
\[
A^{T}Ax - A^{T}b = 0
\]
then we can use \textit{Conjugate gradient} method to minimize the function
\[
f(x) = \frac{1}{2}x^{T}A^{T}Ax - x^{T}A^{T}b
\]

We remember that \begin{itemize}
\item \textbf{A-orthogonal}: $x, y \in R^{m}$ are A-orthogonal if $x^{T}Ay = 0$
\item \textbf{A-norm}: the A-norm of $x \in R^{m}$ is $\|x\|_{A} := (x^{t}Ax)^{\frac{1}{2}}$. Note that if A is \textit{positive definite}, then the A-norm is $\geq 0$ (it is $0$ only for the $0$ vector)
\end{itemize}
Let $x_{k}$ be the parameter vector after the $k$-th iteration, then the residual vector is $r_{k} = b - Ax_{k}$ and the negative gradient $g_{k} = -\nabla f(x_{k}) = A^{T}b - A^{T}Ax = A^{T}r_{k}$. The step size $\alpha_{k}$ can be computed in a similar way as in the standard CG method, but here the denominator is $d_{k}^{T}A^{T}Ad_k = \|Ad_{k}\|^2$ and so it can be obtained without calculating $A^{T}A$.
\\To update the residual, we use the fact that $r_{k} = b - Ax_{k} = [x_{k} = x_{k-1} + \alpha_{k}d_{k-1}] = b - A(x_{k-1} + \alpha_{k}d_{k-1}) = [b-Ax_{k-1} = r_{k-1}] = r_{k-1} - \alpha_{k}Ad_{k-1}$. The trick about conjugate gradient is that at each step we generate a new search direction, which is not exactly the residual, but is the residual modified to be $A^{T}A$-orthogonal to the previous search direction. In this way, $\beta_{k}$ is a scalar that is chosen s.t. $d_{k}$ is $A^{T}A$-orthogonal to $d_{k-1}$.

\subsection{Second algorithm: QR factorization with Householder Reflectors}\label{subsec:qr-factorization-with-householder-reflectors}
To solve the Least Square Problem, we use the \textit{thin QR factorization} with all the optimization seen (fast Householder-vector product, cancellation problem resolution, manually changing known entries) in order to factorize $A$ as $Q_{1}R_{1}$.
We use a variant of the \textit{thin QR factorization} where we do not form the matrix $Q$, but we keep the Householder vector $u_{k}$ to perform implicit product with $Q$ and $Q^{T}$.
With this factorization, we can write $\|Ax - b\|$ as $\left\lVert \begin{bmatrix} R_{1}x - Q_{1}^{T}b \\ Q_{2}^{T}b\end{bmatrix} \right\lVert$
and now we can chose $x$ such that $R_{1}x - Q_{1}^{T}b = 0$, which is $x = R_{1}^{-1}Q_{1}^{T}b$ (if $R_{1}$ is invertible).
We can write $x = R^{-1}(Q^{T}b) = R^{-1}(((I-2v_{1}v_{1}^{T})(I-2v_{2}v_{2}^{T})\dots(I-2v_{m}v_{m}^{T}))^{T}b)$
Finally we should have $x  = \arg\!\min_{x \in  \mathds{R}^n}\|Ax - b\| = R_{1}^{-1}Q_{1}^{T}b$ and $\|Ax - b\| = \|Q_{2}^{T}b\|$.


\section{What to expect from the algorithms}\label{sec:what-to-expect-from-the-algorithms}

\subsection{Conjugate Gradient}\label{subsec:conjugate-gradient}
Our system $Ax = b$ is equivalent to the system $Bx = c$, where $B = A^{T}A$ is symmetric and positive definite and $c = A^{T}b$. However, from a numerical point of view, the two systems are different, because of rounding-off errors that occur in joining the product $A^{T}A$. But since our algorithm does not involve the computation of $B = A^{T}A$, then we have the same convergence result as in the standard CG method. In particular we know that this method gives the solution in $m$ steps if no rounding-off error occurs, where $m$ is the size of $B$, namely the smallest size of our original rectangular matrix $A$. In realistic cases, the absence of rounding-off errors cannot be assured and so we need more than $m$ step or even to restart the algorithm many times. Also the basic relations concerning convergence ($<r_i,r_k>\,= 0, <Ad_i,d_k>\,= 0$ for $i \ne k$) will not be satisfied exactly and therefore $x_m$ will not be as good an estimate the exact solution as desired. From \cite{hestenes1952methods} follows that the larger the ratios $\alpha_i / \alpha_{i-1}$, the more rapidily the rounding-off errors accumulate. Moreover, since $\alpha_i$ lie on the range
\[
1 / \lambda_{max} < \alpha_i < 1 / \lambda_{min}
\] 
the ratio $\rho = \lambda_{max}/\lambda_{min}$ gives us an upper bound of the critical ratio  $\alpha_i / \alpha_{i-1}$ which determines the stability of the entire process. So we would like to have $\rho$ near one, that means our matrix is near multiple of the identity and then the CG method is relatively stable. 

The algorithm performs at every step two multiplications  (one is enough if we compute it and store it) of $A$ with the $m$-vector $d_i$ $(O(m^2n))$ and one multiplication of $A^{T}$ with the $n$-vector $r_i$ $(O(n^2m))$. 
So, the total cost is $m(O(m^2n) + O((n^2m)) + O(m^2))$, where $O(m^2)$ comes from the scalars product between $m$-vectors to compute the norms for every steps.

The convergence of the Conjugate Gradient Method depends on the maximum eigenvalue $\lambda_{max}$ and the minimum eigenvalue $\lambda_{min}$, and CG converges with rate 
\\$\|x-x_{k}\| \leq \left\lVert\frac{\sqrt{\lambda_{max}}-\sqrt{\lambda_{min}}}{\sqrt{\lambda_{max}}+\sqrt{\lambda_{min}}}\right\rVert^{k} \leq \|x - x_{0}\|$

\subsection{QR with HouseHolder}\label{subsec:qr-with-householder}
The QR factorization has a cancellation problem during the Householder reflector construction that is easily fixed by summing first entry and norm instead of subtract it.
\\Apart from that, since every step is \textit{backward stable}, the factorization algorithm is \textit{backward stable}: the computed $Q$, $R$ are the exact result of $qr(A + \Delta A)$ where $ \left\lVert \Delta A \right\rVert \leq O(u)\left\lVert A \right\rVert$, so we only have intrinsic representation errors.
\\The computational cost for thin QR factorization is $2mn^2 - \frac{2}{3}n^3 + O(mn)$ flops, which represents two cases: if $m \approx n$, we have that the cost is $\frac{4}{3}n^3$, if $m \gg n$ the cost scales like $2mn^2$, so it scales linearly with respect to the biggest dimension of A .
\\Before we said that $x = R_{1}^{-1}Q_{1}^{T}b$, but to be able to calculate it we need $R_{1}$ to be invertible.
\\We know that $A$ has full column rank $\iff Az \neq 0 \forall z \neq 0 \iff z^{T}A^{T}Az = \left\lVert Az \right\rVert^{2} \forall z \neq 0 \iff A^{T}A$ is positive definite $\iff$ all eigenvalues of $A^{T}A$ are $> 0\iff $0 is not an eigenvalue of $A^{T}A \iff A^{T}A = (Q_{1}R_{1})^{T}Q_{1}R_{1} = R_{1}^{T}Q_{1}^{T}Q_{1}R_{1} = R_{1}^{T}R_{1}$ is invertible $\iff det(A^{T}A) = det(R_{1}^{T})det(R_{1}) \neq 0 \iff det(R_{1}) \neq 0 \iff R_{1}$ is invertible.
\\So if $A$ has full column rank, $\min_{x \in \mathds{R}^n}\|Ax-b\|$ has solution and it is unique.
\\$R_{1}$ is invertible also if all elements on its diagonal are $\neq 0$.
The cost to solve $x = R_{1}^{-1}(Q_{1}^{T})b$ is a multiplication $c = (Q_{1}^{T})b$, which costs $O(mn)$, and the resolution of the triangular system $R_{1}x = c$ with back-substitution, which costs $O(n^{2})$.
Anyway, the overall cost $O(mn) + O(n^{2})$ is negligible with respect to the cost $O(mn^{2})$ to compute $Q_{1}, R_{1}$.
This factorization algorithm is bandwidth heavy and not parallelizable, as every reflection that produces a new zero element changes the entirety of both Q and R matrices

\section{Input data}\label{sec:input-data}
Our input data is the matrix A, which is the tall thin matrix used as input in the \textit{ML-cup 2019--2020} competition.
Its shape is $(1765, 20)$, we augmented it with few functions of the features of the dataset:
\begin{itemize}
	\item \textbf{21\textsuperscript{th} column}: logarithm of the absolute value of the 1\textsuperscript{st} column
	\item \textbf{22\textsuperscript{th} column}: product of 2\textsuperscript{nd}, 3\textsuperscript{rd} and 4\textsuperscript{th} columns
	\item \textbf{23\textsuperscript{th} column}: 5\textsuperscript{th} column to square
\end{itemize}
So the final shape of $A$ is $(1765, 23)$.
    The condition number of the matrix $A$ (calculated with \textit{numpy.linalg.cond}, which does SVD and $\frac{\sigma_{1}}{\sigma_{n}}$) is $\approx 400000 (4 \times 10^{5})$, while the condition number of a random matrix with the same dimension and same range of values (min and max of the 2 matrices are the same) is $\approx 1.7$.
    Those values doesn't change with respect to the augmented columns, because the augmented columns are not linear combination of other columns.
    We can conclude that the matrix $A$ is ill conditioned, so it is almost singular and the solution of our problem could be not accurate.
    Anyway, we know that the solution of the least square problem exists and it is unique, because $A$ is a full column rank matrix because $rank(A) = n = 23$, where rank is calculated as the number of singular values $> \max(\sigma_{i}) \times max(m, n) \times eps$, which is the same approach used in MATLAB.

\bibliography{bibliography}
\bibliographystyle{unsrt}

\end{document}
