\documentclass{article}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{ dsfont }
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{graphicx}

% Questa linea raddoppia lo spazio tra una riga e l'altra per tutto il documento %
\doublespacing

\begin{document}
% Abstract %
\thispagestyle{plain}
\begin{center}
    \Large
    \textbf{Computational Mathematics for Learning and Data Analysis}

    \vspace{0.4cm}
    \large
    2019 / 2020

    \vspace{0.4cm}
    \textbf{Poggiali Alessandro, Berti Stefano}

    \vspace{0.9cm}
\end{center}

% Nuova pagina %
\newpage
% Setting the stage %
\section{Setting the stage}\label{sec:setting-the-stage}

\subsection{The problem: Least Squares}\label{subsec:linear-square-problem}
Our problem is to find an array x such that \\\\
\centerline{$\min_{x \in \mathds{R}^n}\|Ax-b\|$} \\\\
holds, where
\begin{itemize}
	\item $A$ is a \textit{tall thin matrix} (so it is a matrix $A\in M(m, n, R)$ where $m \gg n$)
	\item $b$ is a vector of real number
	\item $\|.\|$ is the \textit{2-norm} or \textit{Euclidean Norm}: $\|x\| := \sqrt{\sum_{i=1}^n x_i}$
\end{itemize}
and so to find the closest vector to $b$ inside the hyperplane $Im(A)$.

% Conjugate gradiend method %
\subsection{First algorithm: Conjugate Gradient Method}\label{subsec:conjugate-gradient-method}
In general, if a square matrix $B = B^{T}$ is positive definite, then we can find a solution to $Bx = c$ by minimizing the (strictly convex) function 
\[
f(x) = \frac{1}{2}x^{T}Bx - c^{T}x \tag{1} \label{eq:cgfunction}
\]
(that is equal to set $\nabla f(x) = Bx - c = 0$). However our matrix $A$ is a \emph{tall thin matrix} and since in this case finding a solution to  $Ax = b$ is equivalent to solving the normal equations
\[
A^{T}Ax - A^{T}b = 0
\]
then we can use \textit{Conjugate gradient} method to minimize the function
\[
f(x) = \frac{1}{2}x^{T}A^{T}Ax - x^{T}A^{T}b
\]
From now on we set $B = A^{T}A$ and $c = A^{T}b$ so our function to minimize become exactly the \eqref{eq:cgfunction}, where $B$ is positive definite. 
\\We remember that \begin{itemize}
\item \textbf{B-orthogonal}: $x, y \in R^{m}$ are B-orthogonal if $x^{T}By = 0$
\item \textbf{B-norm}: the B-norm of $x \in R^{m}$ is $\|x\|_{B} := (x^{t}Bx)^{\frac{1}{2}}$. Note that if B is \textit{positive definite}, then the B-norm is $\geq 0$ (it is $0$ only for the $0$ vector)
\end{itemize}
Let $x_{k}$ be the parameter vector after the $k$-th iteration, then the residual vector is $r_{k} = b - Ax_{k}$ and the negative gradient $g_{k} = -\nabla f(x_{k}) = c - Bx = A^{T}b - A^{T}Ax = A^{T}r_{k}$. The step size $\alpha_{k}$ can be computed in a similar way as in the standard CG method, but here the denominator is $d_{k}^{T}Bd_k= d_{k}^{T}A^{T}Ad_k = \|Ad_{k}\|^2$ and so it can be obtained without calculating $B = A^{T}A$.
\\To update the residual, we use the fact that $r_{k} = b - Ax_{k} = [x_{k} = x_{k-1} + \alpha_{k-1}d_{k-1}] = b - A(x_{k-1} + \alpha_{k-1}d_{k-1}) = [b-Ax_{k-1} = r_{k-1}] = r_{k-1} - \alpha_{k-1}Ad_{k-1}$. The trick about conjugate gradient is that at each step we generate a new search direction, which is not exactly the residual, but is the residual modified to be $B$-orthogonal to the previous search direction. In this way, $\beta_{k}$ is a scalar that is chosen s.t. $d_{k}$ is $B$-orthogonal to $d_{k-1}$.

% QR factorization method %
\subsection{Second algorithm: QR factorization with Householder Reflectors}\label{subsec:qr-factorization-with-householder-reflectors}
To solve the Least Square Problem, we use the \textit{thin QR factorization} with all the optimization seen (fast Householder-vector product, cancellation problem resolution, manually changing known entries) in order to factorize $A$ as $Q_{1}R_{1}$.
We use a variant \cite{nla} of the \textit{thin QR factorization} where we do not form the matrix $Q$, but we keep the Householder vector $u_{k}$ to perform implicit product with $Q$ and $Q^{T}$.
With this factorization, we can write $\|Ax - b\|$ as $\left\lVert \begin{bmatrix} R_{1}x - Q_{1}^{T}b \\ Q_{2}^{T}b\end{bmatrix} \right\lVert$
and now we can chose $x$ such that $R_{1}x - Q_{1}^{T}b = 0$, which is $x = R_{1}^{-1}Q_{1}^{T}b$ (if $R_{1}$ is invertible).
We can write $x = R^{-1}(Q^{T}b) = R^{-1}(((I-2v_{1}v_{1}^{T})(I-2v_{2}v_{2}^{T})\dots(I-2v_{m}v_{m}^{T}))^{T}b)$
Finally we should have $x  = \arg\!\min_{x \in  \mathds{R}^n}\|Ax - b\| = R_{1}^{-1}Q_{1}^{T}b$ and $\|Ax - b\| = \|Q_{2}^{T}b\|$.

\newpage
\section{What to expect from the algorithms}\label{sec:what-to-expect-from-the-algorithms}

\subsection{Conjugate Gradient}\label{subsec:conjugate-gradient}
Our system $Ax = b$ is equivalent to the system $Bx = c$, where $B = A^{T}A$ is symmetric and positive definite and $c = A^{T}b$. However, from a numerical point of view, the two systems are different, because of rounding-off errors that occur in joining the product $A^{T}A$. But since our algorithm does not involve the computation of $B = A^{T}A$, then we have the same convergence result as in the standard CG method. In particular we know that this method gives the solution in $n$ steps if no rounding-off error occurs, where $n$ is the size of $B$, namely the smallest size of our original rectangular matrix $A$. In realistic cases, the absence of rounding-off errors cannot be assured and so we need more than $n$ step or even to restart the algorithm many times. Also the basic relations concerning convergence ($<r_i,r_k>\,= 0, <Ad_i,d_k>\,= 0$ for $i \ne k$) will not be satisfied exactly and therefore $x_n$ will not be as good an estimate the exact solution as desired. From \cite{hestenes1952methods} follows that the larger the ratios $\alpha_i / \alpha_{i-1}$, the more rapidily the rounding-off errors accumulate. Moreover, since $\alpha_i$ lie on the range
\[
1 / \lambda_{max} < \alpha_i < 1 / \lambda_{min}
\] 
the ratio $\rho = \lambda_{max}/\lambda_{min}$ gives us an upper bound of the critical ratio  $\alpha_i / \alpha_{i-1}$ which determines the stability of the entire process. So we would like to have $\rho$ near one, that means our matrix is near multiple of the identity and then the CG method is relatively stable. 

The algorithm performs at every step two multiplications  (one is enough if we compute it and store it) of $A$ with the $n$-vector $d_i$ $(O(mn))$ and one multiplication of $A^{T}$ with the $m$-vector $r_i$ $(O(nm))$. 
So, the total cost is $n(O(mn) + O((nm)) + O(n)) \approx O(n^2m)$, where $O(n)$ comes from the scalars product between $n$-vectors to compute the norms for every steps.

The convergence of the Conjugate Gradient Method depends on the maximum eigenvalue $\lambda_{max}$ and the minimum eigenvalue $\lambda_{min}$, and CG converges with rate 
\\$\|x-x_{k}\| \leq \left\lVert\frac{\sqrt{\lambda_{max}}-\sqrt{\lambda_{min}}}{\sqrt{\lambda_{max}}+\sqrt{\lambda_{min}}}\right\rVert^{k} \leq \|x - x_{0}\|$

\subsection{QR with HouseHolder}\label{subsec:qr-with-householder}
The QR factorization has a cancellation problem during the Householder reflector construction that is easily fixed by summing first entry and norm instead of subtract it.
\\Apart from that, since every step is \textit{backward stable}, the factorization algorithm is \textit{backward stable}: the computed $Q$, $R$ are the exact result of $qr(A + \Delta A)$ where $ \left\lVert \Delta A \right\rVert \leq O(u)\left\lVert A \right\rVert$, so we only have intrinsic representation errors.
\\By looking at the residual $\left\lVert A x - b\right\rVert$, we can't say if our computed $x$ is close to the true solution $x^{*}$: the residual can be arbitrarily large because it depends on the problem.
The computed $x$ solves the modified problem $\min \left\lVert Ax - (b + Q_{1}r)\right\rVert$, and $x$ is close to the true solution $x^{*}$ if the residual of the top block $r_{1} = Q_{1}^{T}(Ax - b)$ is close to 0.
Moreover, $\frac{\left\lVert x - x^{*} \right\rVert}{ \left\lVert x^{*} \right\rVert} \leq k_{rel, b\rightarrow x}\frac{\left\lVert r \right\rVert}{\left\lVert b \right\rVert}$.
\\The computational cost for thin QR factorization is $2mn^2 - \frac{2}{3}n^3 + O(mn)$ flops, which represents two cases: if $m \approx n$, we have that the cost is $\frac{4}{3}n^3$, if $m \gg n$ the cost scales like $2mn^2$, so it scales linearly with respect to the biggest dimension of A .
\\Before we said that $x = R_{1}^{-1}Q_{1}^{T}b$, but to be able to calculate it we need $R_{1}$ to be invertible.
We know that $A$ has full column rank $\iff Az \neq 0 \forall z \neq 0 \iff z^{T}A^{T}Az = \left\lVert Az \right\rVert^{2} \forall z \neq 0 \iff A^{T}A$ is positive definite $\iff$ all eigenvalues of $A^{T}A$ are $> 0\iff $0 is not an eigenvalue of $A^{T}A \iff A^{T}A = (Q_{1}R_{1})^{T}Q_{1}R_{1} = R_{1}^{T}Q_{1}^{T}Q_{1}R_{1} = R_{1}^{T}R_{1}$ is invertible $\iff det(A^{T}A) = det(R_{1}^{T})det(R_{1}) \neq 0 \iff det(R_{1}) \neq 0 \iff R_{1}$ is invertible.
So if $A$ has full column rank, $\min_{x \in \mathds{R}^n}\|Ax-b\|$ has solution and it is unique.
$R_{1}$ is invertible also if all elements on its diagonal are $\neq 0$.
\\The cost to solve $x = R_{1}^{-1}(Q_{1}^{T})b$ is a multiplication $c = (Q_{1}^{T})b$, which costs $O(mn)$, and the resolution of the triangular system $R_{1}x = c$ with back-substitution, which costs $O(n^{2})$.
Anyway, the overall cost $O(mn) + O(n^{2})$ is negligible with respect to the cost $O(mn^{2})$ to compute $Q_{1}, R_{1}$.
This factorization algorithm is bandwidth heavy and not parallelizable, as every reflection that produces a new zero element changes the entirety of both Q and R matrices

\section{Input data}\label{sec:input-data}
Our input data is the matrix A, which is the tall thin matrix used as input in the \textit{ML-cup 2019--2020} competition.
Its shape is $(1765, 20)$, we augmented it with few functions of the features of the dataset:
\begin{itemize}
	\item \textbf{21\textsuperscript{th} column}: logarithm of the absolute value of the 1\textsuperscript{st} column
	\item \textbf{22\textsuperscript{th} column}: product of 2\textsuperscript{nd}, 3\textsuperscript{rd} and 4\textsuperscript{th} columns
	\item \textbf{23\textsuperscript{th} column}: 5\textsuperscript{th} column to square
\end{itemize}
So the final shape of $A$ is $(1765, 23)$.
    The condition number of the matrix $A$ (calculated with \textit{numpy.linalg.cond}, which does SVD and $\frac{\sigma_{1}}{\sigma_{n}}$) is $\approx 400000 (4 \times 10^{5})$, while the condition number of a random matrix with the same dimension and same range of values (min and max of the 2 matrices are the same) is $\approx 1.7$.
    We can conclude that the matrix $A$ is ill conditioned, so it is almost singular and the solution of our problem could be not accurate.
    Anyway, we know that the solution of the least square problem exists and it is unique, because $A$ is a full column rank matrix ($rank(A) = n = 23$), where rank is calculated as the number of singular values $> \max(\sigma_{i}) \times max(m, n) \times eps$, which is the same approach used in MATLAB .

\section{Code}\label{sec:code}
The code has been implemented in \textit{Python}, we used \textit{Numpy} for array manipulation.
\subsection{Conjugate gradient method Code}
The Algorithm 1 performs the CG method applied to our function \eqref{eq:cgfunction}, starting from an initial guess of $x$. For each iteration we store the pruduct $Ad_k$ in a variable so that it can be computed only once insted of twice.
% CG pseudocode
\makeatletter
\makeatother
\begin{algorithm}
\caption{LS resolution with CG method}
\begin{algorithmic}[1]
\Function{$x \gets A, b$}{}
\State $x_0 \gets starting\,guess$
\State $r_0 \gets b -  Ax_0$
\State $g_0 \gets A^{T}r_0$ 
\State \emph{for i = 1:m}:
\State \quad\emph{if $g_{k-1} = 0$}:
\State\quad\quad\textit{return $x_{k-1}$}
\State \quad\emph{if $i > 1$}:
\State\quad\quad{$\beta_{k} = - \|g_{k-1}\|^2 / \|g_{k-2}\|^2$}
\State \quad\emph{if $i = 1$}: 
\State\quad\quad{$d_{k} = g_{0}$}
\State \quad\emph{else}: 
\State\quad\quad{$d_{k} = g_{k-1} - \beta_{k}d_{k-1}$}
\State\quad $\alpha_{k} = \|g_{k-1}\|^2 / \|Ad_{k}\|^2$
\State\quad $x_{k} = x_{k-1} + \alpha_{k}d_{k}$
\State\quad $r_{k} = r_{k-1} - \alpha_{k}Ad_{k}$
\State\quad $g_{k} = A^{T}r_{k}$
\State\textit{return $x_{m}$}
\EndFunction
\end{algorithmic}
\end{algorithm}
\subsection{QR factorization method code}\label{subsec:qr-code}
In line 6 of Algorithm 4, we used \textit{scipy.linalg.lapack.dtrtri} to compute the inverse of \textit{R}, which is the optimized method for upper-triangular matrices.
 % Householder reflector pseudocode
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
\begin{algorithm}
\caption{Householder reflector}
\begin{algorithmic}[1]
\Function{$[v, s] \gets householder\_vector(x)$}{}
\State $s \gets -sign(x[0]) \times norm(x)$
\State $v \gets x$
\State $v[0] \gets v[0] -s$
\State $v \gets \frac{v}{norm(v)}$
\State \textit{return v, s}
\EndFunction
\end{algorithmic}
\end{algorithm}
% QR factorization pseudocode
\makeatletter
\makeatother
\begin{algorithm}
\caption{QR factorization with Householder Reflectors}
\begin{algorithmic}[1]
\Function{$[V, R] \gets qr\_factorization(A)$}{}
\BState \emph{top}:
\State $\textit{[m, n]} \gets \textit{size(A)}$
\State $\textit{V} \gets \textit{emptyList}$
\BState \emph{loop}:
\State \emph{for j = 1:min(m-1, n)}:
\State \quad$\textit{[v, s]} \gets \textit{householder\_vector(A(j:end, j))}$
\State \quad$\textit{A[j, j]} \gets \textit{s}$
\State \quad$\textit{A[j+1:end, j]} \gets \textit{0}$
\State \quad$\textit{A[j:end, j:end]} \gets \textit{A[j:end, j:end]} - 2\times v \times (v^{T} \times \textit{A[j:end, j+1:end])}$
\State \quad$\textit{V} \gets \textit{V.insert(v)}$
\BState \emph{end}:
\State $\textit{R = A}$
\State \textit{return V, R}
\EndFunction
\end{algorithmic}
\end{algorithm}
% Resolution pseudocode
\makeatletter
\makeatother
\begin{algorithm}
\caption{LS resolution with QR factorization}
\begin{algorithmic}[1]
\Function{$x \gets A, b$}{}
\State $\textit{[V, R]} \gets \textit{qr\_factorization(A)}$
\State $x \gets b$
\State \emph{for i = 1:n}:
\State \quad$x[i:end] \gets x[i:end] - 2 \times v_{i} \times (v_{i}^{T} \times x[i:end])$
\State $x \gets R^{-1}x$
\State \textit{return x}
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{Experimental set up}\label{sec:experimental-set-up}
We are going to compare the two methods over the ML-cup in terms of time spent, memory usage and quality of the solution.
We will look how much our solutions are good in terms of residual and how much we got close to the true solution.
We will check the condition angle of the problem, to understand how much our solution is stable, and we will compare it with a random matrix with the same dimension and range of values of the ML-cup one.
Finally, we will test which is the maximum problem dimension computable for the two methods with a given RAM .
Moreover, for the QR factorization, we are going to test how the computational cost scales varying the dimension $m$ over the 3 matrices $m \times m$, $m \times n$ with $m > n$ and $m \times n$ with $m \gg n$.

\section{Results}\label{sec:results}
For the time results we averaged 1000 run using \textit{time.monotonic\_ns()}, and we removed the best and worst 100 ones.
With space we mean the additional number of bytes used by the methods in addition to the inputs, we measured it with \textit{numpy.array.nbytes}.
                \begin{table}[h!]
                    \begin{center}
                        \caption{Results for ML-cup}
                        \label{tab:table4}
                        \begin{tabular}{l|c|c|c|r}
                            \textbf{method} & \textbf{Time [ns]} & \textbf{Space [bytes]} & $\textbf{Ax - b}$ & $\textbf{Q}^{T}\textbf{(Ax - b)}$\\
                            \hline
                                CG & 0 & 0 & 0 & 0\\
                                QR & 6298970 & 4416 & 348.20 & 2.79e-07
                        \end{tabular}
                    \end{center}
                \end{table}
The space used by the QR method is the number of bytes needed to save the square upper triangular matrix R ($23 \times 23$ floats) and to save the n $v_{i}$'s vector of floats with length in [1765, 1743].
The computation to calculate x in done "in place", so no more space is required.
The condition angle of this problem (calculated as $arccos\frac{\left\lVert A x \right\rVert}{\left\lVert b \right\rVert}$) is 0.45.
As we can see, the solutions are similar in terms of residual, it can be observed that both are close to the true solution, because the value $Q^{T}(Ax - b)$ is close to zero for both, but the residual is big anyway.
This happens because, as said before, the problem is ill-conditioned, so the best solution that we can find is still far away to give us a residual $\left\lVert Ax - b \right\rVert$ close to zero.
        \begin{figure}
            \includegraphics[width=\linewidth]{../results/square.png}
            \caption{QR method: Computational time w.r.t. dimension n for square matrix $n \times n$}
        \end{figure}
        \begin{figure}
            \includegraphics[width=\linewidth]{../results/little_m.png}
            \caption{QR method: Computational time w.r.t. dimension m for tall thin matrix $m \times n$ with $m > n$}
        \end{figure}
        \begin{figure}
            \includegraphics[width=\linewidth]{../results/big_m.png}
            \caption{QR method: Computational time w.r.t. dimension m for tall thin matrix mxn with $m \gg n$}
        \end{figure}

\bibliography{bibliography}
\bibliographystyle{unsrt}

\end{document}
